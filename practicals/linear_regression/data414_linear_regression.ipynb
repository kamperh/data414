{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Herman Kamper, 2020-2021. Licensed under [CC BY-SA 4.0](http://creativecommons.org/licenses/by-sa/4.0/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear regression on simulated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When implementing a new algorithm, it is useful to first develop it in a setting where you know what the answer should be. This helps to make sure that your implementation is correct before applying it in the (more complicated) setting where you actually want to use it. In this section we follow this principle by using simulated data where we have control over how the data is generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Simple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by developing our understanding of linear regression by using simulated data with one input variable. We use the simple linear regression model:\n",
    "\n",
    "$$f(x; w_0, w_1) = w_0 + w_1x$$\n",
    "\n",
    "In class we showed that the least squares estimates for the coefficients of this model is given by:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{w}_0 &= \\bar{y} - w_1\\bar{x} \\\\\n",
    "\\hat{w}_1 &= \\frac{\\sum_{n = 1}^N (x^{(n)} - \\bar{x})(y^{(n)}- \\bar{y})}{\\sum_{n = 1}^N (x^{(n)} - \\bar{x})^2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\bar{x} = \\frac{1}{N}\\sum_{n = 1}^N x^{(n)}$ and similarly for $\\bar{y}$.\n",
    "\n",
    "Below we generate $N$ observations $\\{ (x^{(n)}, y^{(n)}) \\}_{n = 1}^N$ of inputs $x$ with corresponding outputs $y$. In the space provided, write the code to answer the following questions.\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "- Plot the simulated data. You can use `plt.scatter`.\n",
    "- Write code to determine the least squares estimates of $w_0$ and $w_1$ using the equations above.\n",
    "- Plot a line showing this least squares fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate data\n",
    "N = 100\n",
    "X = 2*np.random.rand(N, 1)\n",
    "y = 4 + 3*X + np.random.randn(N, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Answer: Add code and new cells here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the input variables were multi-dimensional vectors $\\mathbf{x}$, we could find the least squares estimate of the vector of weights $\\mathbf{w}$ using\n",
    "\n",
    "$$\\hat{\\mathbf{w}} = (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{y}$$\n",
    "\n",
    "**Question:** Implement this equation directly using NumPy to again find the least squares estimates $\\hat{w}_0$ and $\\hat{w}_1$. How do these values compare to those obtained above?  \n",
    "*Hints:* Useful NumPy functions include `np.concatenate` (or the shorter `np.c_`), `np.dot` and `np.linalg.inv`. Remember to take special care to deal with the bias weight $w_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Answer: Add code and new cells here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "    \n",
    "- Go back to the top of Section 1.1, and change the number of simulated data points from $N = 100$ to $N = 1\\,000\\,000$. Then re-run all the cells.\n",
    "- How does the estimated coefficients $\\hat{w}_0$ and $\\hat{w}_1$ now compare to the code used to generate the simulated data?\n",
    "- How does the speed of the scalar implementation (start of Section 1.1) compare to that of the vectorised implementation (end of Section 1.1)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extend standard linear regression to incorporate non-linear relationships by including polynomial features, sometimes referred to as *polynomial regression*. We can still use the linear regression code to fit this model, since the model is still linear in its parameters.\n",
    "\n",
    "Consider the following model:\n",
    "\n",
    "$$f(x; w_0, w_1) = w_0 + w_1x + w_2x^2$$\n",
    "\n",
    "Below we generate a different dataset of $N$ observations $\\{ (x^{(n)}, y^{(n)}) \\}_{n = 1}^N$ of inputs $x$ with corresponding outputs $y$.\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "- Plot the simulated data.\n",
    "- Write code to determine the least squares estimates of $\\mathbf{w}$.\n",
    "- Plot a curve showing the least squares fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "X = 6*np.random.rand(N, 1) - 3\n",
    "y = 2 + X + 0.5 * X**2 + np.random.randn(N, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Answer: Add code and new cells here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is relatively easy to construct the polynomial functions using Python directly, but it might become cumbersome when dealing with multi-dimensional inputs and higher-order polynomial features. Luckily scikit-learn provides functionality for easily constructing high-degree polynomial features, for instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "print(\"Original: \", X[0])\n",
    "print(\"Polynomial: \", X_poly[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn also has functionality to directly fit a linear regression model, for instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "print(\"w_0: {:.4f}\".format(model.intercept_[0]))\n",
    "print(\"w_1: {:.4f}\".format(model.coef_[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LinearRegression` scikit-learn class is itself very similar to the NumPy function `np.linalg.lstsq`, which you could also call directly.\n",
    "\n",
    "**Question:** Now fit a linear regression model with $20^{\\text{th}}$ order polynomial features and plot the fit. You can use any of the scikit-learn functions or classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Answer: Add code and new cells here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "- Go back to the top of Section 1.2, and change the number of simulated data points from $N = 100$ to $N = 100\\,000$. Then re-run all the cells.\n",
    "- How does the estimated coefficients $\\hat{w}_0$, $\\hat{w}_1$ and $\\hat{w}_2$ now compare to the code used to generate the simulated data?\n",
    "- Does the additional data help when fitting the $20^{\\text{th}}$ order polynomial features?\n",
    "- Do you think additional data would help if we tried to fit the linear model $f(x; w_0, w_1) = w_0 + w_1x$ from Section 1.1 to the data in Section 1.2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear regression on real data (with multiple variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Boston* dataset contains median house values for 506 neighbourhoods around Boston. We will try to predict house prices based on 13 input features. In the code below, the dataset is loaded directly using scikit-learn. More details of the 13 features are given [here](https://scikit-learn.org/stable/datasets/index.html#boston-dataset). You can use scikit-learn classes and functions throughout this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Load and visualise data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below loads the data and plots a histogram of prices.\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "- Create a scatter plot of \"price\" against \"per capita crime rate\" for all 506 neighbourhoods.\n",
    "- Now divide the data into training and test sets, with the first 75% of the data used for training and the remaining 25% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "from sklearn.datasets import load_boston\n",
    "data = load_boston()\n",
    "feature_names = data.feature_names\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Histogram of prices\n",
    "plt.hist(data.target)\n",
    "plt.xlabel(\"Price ($1000s)\")\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Answer: Add code and new cells here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Simple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "- Perform simple linear regression where the input $x$ is `crime` and the output $y$ is `price`.\n",
    "- Plot all the data.\n",
    "- Plot only the training data.\n",
    "- On both of the above plots, show the least squares fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Answer: Add code and new cells here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of ways to measure the performance of a regression model on held-out test data. The least squares approach itself uses a squared loss:\n",
    "\n",
    "$$ J = \\sum_{n = 1}^N \\left(y^{(n)} - f(\\mathbf{x}^{(n)}; \\mathbf{w})\\right)^2 $$\n",
    "\n",
    "We can also calculate the *mean squared error* (MSE):\n",
    "\n",
    "$$ \\text{MSE} = \\frac{1}{N} \\sum_{n = 1}^N \\left(y^{(n)} - f(\\mathbf{x}^{(n)}; \\mathbf{w})\\right)^2 $$\n",
    "\n",
    "or the *root-mean-square error* (RMSE):\n",
    "\n",
    "$$ \\text{RMSE} = \\sqrt{ \\frac{1}{N} \\sum_{n = 1}^N \\left(y^{(n)} - f(\\mathbf{x}^{(n)}; \\mathbf{w})\\right)^2 } $$\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "- Calculate the squared loss, MSE and RMSE metrics on the test data.\n",
    "- Calculate the squared loss, MSE and RMSE metrics on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Answer: Add code and new cells here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "- Repeat Section 2.2 but now use $5^{\\text{th}}$ order polynomial features.\n",
    "- How do the metrics compare on the training and test data when using such a high-order polynomial?\n",
    "- How do the trends in the metrics compare to that of the simple linear regression case in Section 2.2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Answer: Add code and new cells here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional question:** Go back and repeat Section 2.2 and 2.3 for the input feature \"average number of rooms per dwelling\". What do you observe? If you write your code carefully, you could probably make it easy to explore simple regression using any of the 13 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Multiple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "- Now perform multiple linear regression using all 13 features as the input $\\mathbf{x}$ and the price as output $y$.\n",
    "- Again calculate the squared loss, MSE and RMS on the training and test data.\n",
    "- How does this compare to the models above?\n",
    "- Print the values of each of the coefficients of the model.\n",
    "- What do these coefficents tell us? For instance, what does it say about the relationship about `price` and `crime`? Or `price` and `rooms`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Answer: Add code and new cells here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "I took parts of Section 2 of the practical from the [SciPy Lecture Notes](https://scipy-lectures.org/packages/scikit-learn/auto_examples/plot_boston_prediction.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
